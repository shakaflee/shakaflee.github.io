---
layout: post
title: 实体机器人版ChatGPT来了！
date: 2023-07-29 05:01:00 +0800
excerpt: 谷歌发布RT-2
tag: news
cover: /assets/images/posts/100083.png
description: 谷歌发布RT-2，实体机器人版ChatGPT来了！
permalink: /100083
---


# {{ page.title }}



7月29日，谷歌旗下的AI研究机构DeepMind发布了RT-2，这是一种新的视觉语言动作模型（VLA）。（地址：https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/）

据悉，RT-2可以从互联网和机器人数据中自动学习，并将这些知识转化为机器人控制的通用指令实现一系列操作动作，同时在思维推理链加持下，RT-2 可执行深度语义推理。例如，让RT-2找出哪些适合砸石头的锤子，哪些能提神的饮料，哪些水果适合减肥的人群等。

根据DeepMind的示例展示，RT-2模型的主要应用场景是集成在实体机器人，使其可以像人类那样思考、推理去执行各种动作任务，就像实体机器人版ChatGPT。RT-2在物流、制造、安保等领域有非常宽广的应用空间。



> RT-2简单介绍

RT-2是在RT-1基础之上研发的，而RT-1是基于Transformer模型，所以，RT-2在技术基因上与ChatGPT有很强的联系，也是能执行高级推理任务的原因之一。（RT-1介绍地址：https://robotics-transformer1.github.io/）

技术原理，RT-2以VLM（高容量视觉语言模型）为基础，将一个或多个图像作为输入，并生成一系列通常代表自然语言文本的标记。此类 VLM 已接受网络大规模数据的训练，以执行视觉问答、图像字幕或对象识别等任务。



为了更好地控制机器人，必须训练其输出动作。DeepMind通过将操作表示为模型输出中的标记（类似于语言标记）来解决这一挑战，并将操作描述为可以由标准自然语言标记器处理的字符串。

这种字符串的示例，可以是机器人动作标记编号的序列，例如“1 128 91 241 5 101 127 217”。



> RT-2 训练中使用的动作字符串的表示形式

该字符串以一个标志开头，指示是继续或终止当前动作，而不执行后续命令。然后更改末端执行器的位置和旋转以及机器人夹具所需延伸的命令。

DeepMind在RT-2使用与 RT-1 中相同的机器人动作离散版本，并表明将其转换为字符串表示，使得可以在机器人数据上训练 VLM 模型，所以此类模型的输入和输出空间不需要改变。

> RT-2 架构和训练

DeepMind针对机器人和网络数据共同微调预先训练的 VLM 模型，生成的模型接收机器人摄像头图像，并直接预测机器人接下来要执行的动作。

> RT-2训练数据

DeepMind在RT-2模型上进行了一系列定性和定量实验，进行了 6,000 多次机器人试验。在探索RT-2的新功能时，首先需要将网络规模数据的知识与机器人的经验相结合的任务，然后定义三类技能：符号理解、推理和人类识别。

每项任务都需要理解视觉语义概念以及执行机器人控制，以掌控操作这些动作的能力，例如，捡起从桌子上掉下来的袋子，将绿色的可乐瓶子与同色物体放在一起的命令等。

从而要求机器人对数据集中，从未见过的物体或场景执行操作任务，将知识从基于网络的数据转化为可操作的实体动作。

RT-2 保留了机器人数据中看到的原始任务的能力，并提高了机器人在以前未见过的场景中的性能，从RT-1的32%提高到了62%，显示了大规模预训练的巨大优势。


此外，DeepMind受LLMs（大语言模型）思维链提示方法的启发，将机器人控制与思维链推理相结合，可在单个模型中学习长期规划和低级技能。尤其是，DeepMind对 RT-2 的变体进行了几百个梯度步骤的微调，提高其联合使用语言和动作的能力，具备理解自然语言的能力。
